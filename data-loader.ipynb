{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json, time, datetime\n",
    "import random\n",
    "import math\n",
    "import sklearn.preprocessing as prep\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "DATA_MARKET = 'data/poloniex/'\n",
    "DATA_TWITTER = 'data/twitter/sentiment/'\n",
    "\n",
    "INPUT_SEQ_LENGTH = 288 # 3 Days - 3*24*60/5\n",
    "OUTPUT_SEQ_LENGTH = 24 # 2 hours\n",
    "\n",
    "class PastSampler:\n",
    "\n",
    "    def __init__(self, N, K, sliding_window = True, step_size=1):\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    "        self.step_size = step_size\n",
    " \n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #Number of samples per row (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1, step=self.step_size).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)    \n",
    "        #print(I)\n",
    "        #print(I.shape)\n",
    "        \n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #Number of features per sample\n",
    "        #print('ci', ci)\n",
    "        #print('B shape', B.shape)\n",
    "        return B[:, :ci], B[:, ci:, 0:1] #Sample matrix, Target matrix\n",
    "\n",
    "\n",
    "def date_to_timestamp(s):\n",
    "    return time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple())\n",
    "    \n",
    "def print_time(unix, msg=''):\n",
    "    print(msg, time.ctime(int(unix)))\n",
    "\n",
    "def split_data(data, s='01/03/2018'):\n",
    "    split_time = date_to_timestamp(s)\n",
    "    train = data.query('date<=@split_time')\n",
    "    test = data.query('date>@split_time')\n",
    "    return train, test\n",
    "\n",
    "def download_data():\n",
    "    # connect to poloniex's API\n",
    "    CURRENCIES = ['USDT_BTC', 'USDT_LTC', 'USDT_ETH', 'USDT_XRP']\n",
    "    url = 'https://poloniex.com/public?command=returnChartData&currencyPair=$C&start=1356998100&end=9999999999&period=300'\n",
    "    urls = [url.replace('$C', c) for c in CURRENCIES]\n",
    "\n",
    "    for i, c in enumerate(CURRENCIES):\n",
    "        with urlopen(urls[i]) as url:\n",
    "            r = url.read()\n",
    "            d = json.loads(r.decode())\n",
    "            df = pd.DataFrame(d)\n",
    "            df = df.drop(columns=['high', 'low', 'open', 'weightedAverage'])\n",
    "            #print(df.columns)\n",
    "            df.to_pickle(DATA_MARKET + c + '.pkl')\n",
    "            print('Successfully downloaded', c)\n",
    "            print_time(min(df['date']), 'MIN:')\n",
    "            print_time(max(df['date']), 'MAX:')\n",
    "            \n",
    "    \n",
    "    df_btc = pd.read_pickle(DATA_MARKET + 'USDT_BTC.pkl')\n",
    "    df_ltc = pd.read_pickle(DATA_MARKET + 'USDT_LTC.pkl')\n",
    "    df_eth = pd.read_pickle(DATA_MARKET + 'USDT_ETH.pkl')\n",
    "    df_xrp = pd.read_pickle(DATA_MARKET + 'USDT_XRP.pkl')\n",
    "    \n",
    "    \n",
    "    #combine all dataframes into one with size of smallest dataframe - discard every other value\n",
    "    count = [min(df_btc.count(numeric_only=True)), min(df_ltc.count(numeric_only=True)), min(df_eth.count(numeric_only=True)), min(df_xrp.count(numeric_only=True))]\n",
    "    count = min(count)\n",
    "    print_time(df_ltc['date'].iloc[-count], 'min date:')\n",
    "\n",
    "    df_btc = df_btc.add_prefix('btc_')\n",
    "    df_eth = df_eth.add_prefix('eth_')\n",
    "    df_ltc = df_ltc.add_prefix('ltc_')\n",
    "    df_xrp = df_xrp.add_prefix('xrp_')\n",
    "\n",
    "    df_all = pd.concat([df_btc.iloc[-count:].reset_index(drop=True), df_eth.iloc[-count:].reset_index(drop=True), df_ltc.iloc[-count:].reset_index(drop=True), df_xrp.iloc[-count:].reset_index(drop=True)], axis=1)\n",
    "    df_all.count(numeric_only=True)\n",
    "\n",
    "    #cuz date column is same for every currency, we will discard others\n",
    "    df_all.head()\n",
    "    df_all['date'] = df_all['btc_date']\n",
    "    df_all = df_all.drop(columns=['btc_date', 'ltc_date', 'eth_date', 'xrp_date'])\n",
    "    df_all.to_pickle(DATA_MARKET + 'combined.pkl')\n",
    "\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"    \n",
    "    price_data = pd.read_pickle(DATA_MARKET + 'combined.pkl')\n",
    "    sentiment_data = pd.read_pickle(DATA_TWITTER + 'btc_expanded.pkl')\n",
    "    \n",
    "    min_date = min(sentiment_data['date'])\n",
    "    max_date = max(sentiment_data['date'])\n",
    "    \n",
    "    price_data = price_data.query('@min_date <= date <= @max_date')\n",
    "    \n",
    "    return pd.merge(price_data, sentiment_data, how='inner', left_on='date', right_on='date')\n",
    "\n",
    "def normalize_fit_transform(X, fields=None):\n",
    "    \"\"\"\n",
    "    Normalize data \n",
    "    \"\"\"\n",
    "    global scaler \n",
    "    scaler = prep.MinMaxScaler()\n",
    "    if fields is not None:\n",
    "        X = scaler.fit_transform(X[fields])\n",
    "    else:\n",
    "        X = scaler.fit_transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "def normalize_transform(X):\n",
    "    if scaler is None:\n",
    "        print('Scaler doesnt exist, please use normalize_fit_transform function first')\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "        return X\n",
    "    \n",
    "def denormalize_1d(data, min_, scale_):\n",
    "    data -= min_\n",
    "    data /= scale_\n",
    "    return data\n",
    "\n",
    "def denormalize_full(data):\n",
    "    if scaler is None:\n",
    "        print('Scaler doesnt exist, please use normalize_fit_transform function first')\n",
    "    else:\n",
    "        X = scaler.inverse_transform(data)\n",
    "        return X\n",
    "\n",
    "def fetch_batch_size_random(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Returns randomly an aligned batch_size of X and Y among all examples.\n",
    "    The external dimension of X and Y must be the batch size (eg: 1 column = 1 example).\n",
    "    X and Y can be N-dimensional.\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0], (X.shape, Y.shape)\n",
    "    idxes = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_out = np.array(X[idxes]).transpose((1, 0, 2))\n",
    "    Y_out = np.array(Y[idxes]).transpose((1, 0, 2))\n",
    "    return X_out, Y_out\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "def prepare_data(input_seq_length, output_seq_length, sliding_window=True, step_size=5):\n",
    "    data = load_data()\n",
    "    train, test = split_data(data)\n",
    "\n",
    "    train = train.drop(columns=['date'])\n",
    "    test = test.drop(columns=['date'])\n",
    "\n",
    "    train, _ = normalize_fit_transform(train)\n",
    "    test = normalize_transform(test)\n",
    "\n",
    "    ps = PastSampler(input_seq_length, output_seq_length, sliding_window=True, step_size=5)\n",
    "\n",
    "    X_train, Y_train = ps.transform(train[:,None,:])\n",
    "    X_test, Y_test = ps.transform(test[:,None,:])\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def generate_data_tf(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    global Y_train\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global Y_test\n",
    "    \n",
    "    if len(Y_test) == 0:\n",
    "        X_train, Y_train, X_test, Y_test = prepare_data(INPUT_SEQ_LENGTH, OUTPUT_SEQ_LENGTH, sliding_window=True, step_size=5)\n",
    "\n",
    "    if isTrain:\n",
    "        return fetch_batch_size_random(X_train, Y_train, batch_size)\n",
    "    else:\n",
    "        return fetch_batch_size_random(X_test,  Y_test,  batch_size)\n",
    "\n",
    "def generate_data_keras(input_seq_length, output_seq_length):\n",
    "    X_train, Y_train, X_test, Y_test = prepare_data(input_seq_length, output_seq_length, sliding_window=True, step_size=5)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45488, 50, 17) (45488, 10, 1)\n",
      "(1773, 50, 17) (1773, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = generate_data_keras(50, 10)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([450, 363,  74])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxes = np.random.randint(500, size=3)\n",
    "idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN Fri Jan  1 01:55:00 2016\n",
      "MAX Sun Apr  1 00:40:00 2018\n",
      "(236422, 18)\n",
      "ci 864\n",
      "B shape (266, 888, 18)\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "print_time(min(data['date']), 'MIN')\n",
    "print_time(max(data['date']), 'MAX')\n",
    "#print(data.info())\n",
    "ps = PastSampler(INPUT_SEQ_LENGTH, OUTPUT_SEQ_LENGTH, sliding_window=False, step_size=5)\n",
    "\n",
    "data_norm, _scaler = normalize_fit_transform(data)\n",
    "print(data_norm.shape)\n",
    "x, y = ps.transform(data_norm[:,None,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-828d7ceb790b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_denorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\deyan\\tf-workspace\\env\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scale_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deyan\\tf-workspace\\env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 451\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "y_denorm = _scaler.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 864, 18) (266, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00005108 0.00080358 0.00000011 0.00070345 0.00005184 0.00000038\n",
      " 0.00272846 0.00003035 0.00000041 0.30533256 0.00000009 0.00000018\n",
      " 0.00000001 0.54848092 0.00001285 0.20402215 4.7191309  0.5       ]\n",
      "[ 3.20000000e+02  0.00000000e+00  0.00000000e+00  9.14500000e-01\n",
      "  0.00000000e+00  0.00000000e+00  1.64298200e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.65250000e-03  0.00000000e+00  0.00000000e+00\n",
      "  1.45160970e+09 -8.80753846e-01  0.00000000e+00 -1.43142857e+00\n",
      "  7.88091068e-01 -1.00000000e+00]\n",
      "[1.98966873e+04 1.24443795e+03 8.90561191e+06 1.42247000e+03\n",
      " 1.92911964e+04 2.63597001e+06 3.68150478e+02 3.29514590e+04\n",
      " 2.44343746e+06 3.27976999e+00 1.09003565e+07 5.60500361e+06\n",
      " 1.52253600e+09 9.42463636e-01 7.78133333e+04 3.47000000e+00\n",
      " 9.99994493e-01 1.00000000e+00]\n",
      "[   19576.6872996      1244.43795046  8905611.9065042      1421.55549992\n",
      "    19291.19637626  2635970.0134316       366.5074956     32951.45899596\n",
      "  2443437.4649415         3.27511749 10900356.479903    5605003.6082574\n",
      " 70926300.                1.82321748    77813.33333333        4.90142857\n",
      "        0.21190342        2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(_scaler.scale_)\n",
    "print(_scaler.data_min_)\n",
    "print(_scaler.data_max_)\n",
    "print(_scaler.data_range_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*24*60/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, w = generate_data_keras(288, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1697.26904296875"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.nbytes/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3199462890625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.nbytes/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.360107421875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.nbytes/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31549072265625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.nbytes/1024/1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
