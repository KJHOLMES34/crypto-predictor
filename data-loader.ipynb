{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json, time, datetime\n",
    "import random\n",
    "import math\n",
    "import sklearn.preprocessing as prep\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "DATA_MARKET = 'data/poloniex/'\n",
    "DATA_TWITTER = 'data/twitter/sentiment/'\n",
    "DATA_BLOCKCHAIN = 'data/blockchain/'\n",
    "\n",
    "INPUT_SEQ_LENGTH = 288 # 3*24*60/5\n",
    "OUTPUT_SEQ_LENGTH = 24 # 4 hours\n",
    "\n",
    "#DROP_COLUMNS = ['ltc_close', 'ltc_volume', 'ltc_quoteVolume', 'eth_close', 'eth_volume', 'eth_quoteVolume', 'xrp_close', 'xrp_volume', 'xrp_quoteVolume']\n",
    "\n",
    "USE_TWITTER = True\n",
    "USE_BLOCKCHAIN = True\n",
    "\n",
    "TARGET_VARIABLE = 'btc_close'\n",
    "\n",
    "PERIOD = 14400\n",
    "\n",
    "BLOCKCHAIN_FEATURES = [\n",
    "            #'transactions-per-second',\n",
    "            'avg-block-size',  \n",
    "            'cost-per-transaction', \n",
    "            'difficulty', \n",
    "            'hash-rate',\n",
    "            'market-cap',\n",
    "            'median-confirmation-time',\n",
    "            'transaction-fees',\n",
    "            'transaction-fees-usd',\n",
    "            'n-transactions-per-block',\n",
    "            'miners-revenue',\n",
    "            'n-unique-addresses',\n",
    "            'n-transactions',\n",
    "            'n-transactions-total',\n",
    "            #'mempool-growth',\n",
    "            #'mempool-count',\n",
    "            #'mempool-size',\n",
    "            'n-transactions-excluding-popular',\n",
    "            'n-transactions-excluding-chains-longer-than-100',\n",
    "            'output-volume',\n",
    "            'estimated-transaction-volume',\n",
    "            'estimated-transaction-volume-usd']\n",
    "\n",
    "class PastSampler:\n",
    "\n",
    "    def __init__(self, N, K, sliding_window = True, step_size=1):\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    "        self.step_size = step_size\n",
    " \n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #Number of samples per row (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1, step=self.step_size).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)    \n",
    "        #print(I)\n",
    "        #print(I.shape)\n",
    "        \n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #Number of features per sample\n",
    "        #print('ci', ci)\n",
    "        #print('B shape', B.shape)\n",
    "        return B[:, :ci], B[:, ci:, 0:1] #Sample matrix, Target matrix\n",
    "\n",
    "def to_timestamp_full(dates):\n",
    "    try:\n",
    "        return [int(time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())) for s in dates]\n",
    "    except:\n",
    "        print('Problem with dates', dates[1])\n",
    "    \n",
    "def date_to_timestamp(s):\n",
    "    return time.mktime(datetime.datetime.strptime(s, \"%d/%m/%Y\").timetuple())\n",
    "    \n",
    "def print_time(unix, msg=''):\n",
    "    print(msg, time.ctime(int(unix)))\n",
    "\n",
    "def split_data(data, s='01/03/2018'):\n",
    "    split_time = date_to_timestamp(s)\n",
    "    train = data.query('date<=@split_time')\n",
    "    test = data.query('date>@split_time')\n",
    "    return train, test\n",
    "\n",
    "def download_btc_blockchain():\n",
    "    #url = 'https://api.blockchain.info/charts/$C?timespan=1week&sampled=false&format=csv'\n",
    "    url = 'https://api.blockchain.info/charts/$C?timespan=3years&sampled=false&format=csv'\n",
    "\n",
    "    urls = [url.replace('$C', c) for c in BLOCKCHAIN_FEATURES]\n",
    "    data = pd.DataFrame()\n",
    "    for i, c in enumerate(BLOCKCHAIN_FEATURES):\n",
    "        temp_df = pd.read_csv(urls[i], names=['date'] + [BLOCKCHAIN_FEATURES[i]])\n",
    "        if not i:\n",
    "            data = temp_df\n",
    "        else:\n",
    "            data = pd.merge(data, temp_df, how='inner', left_on='date', right_on='date')\n",
    "        print('reading', c, 'with size of', data[c].count())\n",
    "    data['date_readable'] = data['date']    \n",
    "    data['date'] = to_timestamp_full(data['date'])\n",
    "    data.to_pickle(DATA_BLOCKCHAIN + 'btc_combined.pkl')\n",
    "    print('Done...')\n",
    "\n",
    "\n",
    "def download_data(period=300):\n",
    "    # connect to poloniex's API\n",
    "    CURRENCIES = ['USDT_BTC', 'USDT_LTC', 'USDT_ETH', 'USDT_XRP']\n",
    "    url = 'https://poloniex.com/public?command=returnChartData&currencyPair=$C&start=1356998100&end=9999999999&period=' + str(period)\n",
    "    urls = [url.replace('$C', c) for c in CURRENCIES]\n",
    "\n",
    "    for i, c in enumerate(CURRENCIES):\n",
    "        with urlopen(urls[i]) as url:\n",
    "            r = url.read()\n",
    "            d = json.loads(r.decode())\n",
    "            df = pd.DataFrame(d)\n",
    "            #df = df.drop(columns=['high', 'low', 'open', 'weightedAverage'])\n",
    "            #print(df.columns)\n",
    "            df.to_pickle(DATA_MARKET + c + '.pkl')\n",
    "            print('Successfully downloaded', c)\n",
    "            print_time(min(df['date']), 'MIN:')\n",
    "            print_time(max(df['date']), 'MAX:')\n",
    "            \n",
    "    \n",
    "    df_btc = pd.read_pickle(DATA_MARKET + 'USDT_BTC.pkl')\n",
    "    df_ltc = pd.read_pickle(DATA_MARKET + 'USDT_LTC.pkl')\n",
    "    df_eth = pd.read_pickle(DATA_MARKET + 'USDT_ETH.pkl')\n",
    "    df_xrp = pd.read_pickle(DATA_MARKET + 'USDT_XRP.pkl')\n",
    "    \n",
    "    \n",
    "    #combine all dataframes into one with size of smallest dataframe - discard every other value\n",
    "    count = [min(df_btc.count(numeric_only=True)), min(df_ltc.count(numeric_only=True)), min(df_eth.count(numeric_only=True)), min(df_xrp.count(numeric_only=True))]\n",
    "    count = min(count)\n",
    "    print_time(df_ltc['date'].iloc[-count], 'min date:')\n",
    "\n",
    "    df_btc = df_btc.add_prefix('btc_')\n",
    "    df_eth = df_eth.add_prefix('eth_')\n",
    "    df_ltc = df_ltc.add_prefix('ltc_')\n",
    "    df_xrp = df_xrp.add_prefix('xrp_')\n",
    "\n",
    "    df_all = pd.concat([df_btc.iloc[-count:].reset_index(drop=True), df_eth.iloc[-count:].reset_index(drop=True), df_ltc.iloc[-count:].reset_index(drop=True), df_xrp.iloc[-count:].reset_index(drop=True)], axis=1)\n",
    "    df_all.count(numeric_only=True)\n",
    "\n",
    "    #cuz date column is same for every currency, we will discard others\n",
    "    df_all.head()\n",
    "    df_all['date'] = df_all['btc_date']\n",
    "    df_all = df_all.drop(columns=['btc_date', 'ltc_date', 'eth_date', 'xrp_date'])\n",
    "    df_all.to_pickle(DATA_MARKET + 'combined.pkl')\n",
    "\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"    \n",
    "    price_data = pd.read_pickle(DATA_MARKET + 'combined.pkl')\n",
    "\n",
    "    currency = TARGET_VARIABLE.split('_')[0]\n",
    "\n",
    "    sentiment_data = pd.read_pickle(DATA_TWITTER + currency + '_expanded.pkl')\n",
    "    blockchain_data = pd.read_pickle(DATA_BLOCKCHAIN + currency + '_blockchain.pkl')\n",
    "    \n",
    "    \n",
    "    #print(price_data.info())\n",
    "    #print(sentiment_data.info())\n",
    "    #print(blockchain_data.info())\n",
    "    \n",
    "    #sentiment_data = pd.read_pickle(DATA_TWITTER + currency + '_expanded.pkl')\n",
    "    #blockchain_data = pd.read_pickle(DATA_BLOCKCHAIN + currency + '_blockchain.pkl')\n",
    "\n",
    "    #price_data = price_data.drop(columns=DROP_COLUMNS)\n",
    "    \n",
    "    min_date = min(sentiment_data['date'])\n",
    "    max_date = max(sentiment_data['date'])\n",
    "    \n",
    "    price_data = price_data.query('@min_date <= date <= @max_date')\n",
    "    data = price_data\n",
    "\n",
    "    if USE_TWITTER:\n",
    "        data = pd.merge(data, sentiment_data, how='inner', left_on='date', right_on='date')\n",
    "    \n",
    "    if USE_BLOCKCHAIN:\n",
    "        data = pd.merge(data, blockchain_data, how='inner', left_on='date', right_on='date')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def normalize_fit_transform(X, fields=None):\n",
    "    \"\"\"\n",
    "    Normalize data \n",
    "    \"\"\"\n",
    "    global scaler \n",
    "    scaler = prep.MinMaxScaler(feature_range=(0,1))\n",
    "    if fields is not None:\n",
    "        X = scaler.fit_transform(X[fields])\n",
    "    else:\n",
    "        X = scaler.fit_transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "def normalize_transform(X):\n",
    "    if scaler is None:\n",
    "        print('Scaler doesnt exist, please use normalize_fit_transform function first')\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "        return X\n",
    "    \n",
    "def denormalize_1d(data, min_, scale_):\n",
    "    data -= min_\n",
    "    data /= scale_\n",
    "    return data\n",
    "\n",
    "def denormalize_full(data):\n",
    "    if scaler is None:\n",
    "        print('Scaler doesnt exist, please use normalize_fit_transform function first')\n",
    "    else:\n",
    "        X = scaler.inverse_transform(data)\n",
    "        return X\n",
    "\n",
    "def fetch_batch_size_random(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Returns randomly an aligned batch_size of X and Y among all examples.\n",
    "    The external dimension of X and Y must be the batch size (eg: 1 column = 1 example).\n",
    "    X and Y can be N-dimensional.\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0], (X.shape, Y.shape)\n",
    "    idxes = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_out = np.array(X[idxes]).transpose((1, 0, 2))\n",
    "    Y_out = np.array(Y[idxes]).transpose((1, 0, 2))\n",
    "    return X_out, Y_out\n",
    "\n",
    "def fetch_batch_size_random_keras(X, Y, batch_size):\n",
    "    assert X.shape[0] == Y.shape[0], (X.shape, Y.shape)\n",
    "    idxes = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_out = np.array(X[idxes])\n",
    "    Y_out = np.array(Y[idxes])\n",
    "    return X_out, Y_out\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "def prepare_data(input_seq_length, output_seq_length, sliding_window=True, step_size=5):\n",
    "    data = load_data()\n",
    "    cols = [TARGET_VARIABLE] + [col for col in data if col != TARGET_VARIABLE]\n",
    "    data = data[cols]\n",
    "    train, test = split_data(data)\n",
    "\n",
    "    train = train.drop(columns=['date'])\n",
    "    test = test.drop(columns=['date'])\n",
    "\n",
    "    train, _ = normalize_fit_transform(train)\n",
    "    test = normalize_transform(test)\n",
    "\n",
    "    ps = PastSampler(input_seq_length, output_seq_length, sliding_window=True, step_size=step_size)\n",
    "\n",
    "    X_train, Y_train = ps.transform(train[:,None,:])\n",
    "    X_test, Y_test = ps.transform(test[:,None,:])\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def generate_data_tf(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    global Y_train\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global Y_test\n",
    "    \n",
    "    if len(Y_test) == 0:\n",
    "        X_train, Y_train, X_test, Y_test = prepare_data(INPUT_SEQ_LENGTH, OUTPUT_SEQ_LENGTH, sliding_window=True, step_size=5)\n",
    "\n",
    "    if isTrain:\n",
    "        return fetch_batch_size_random(X_train, Y_train, batch_size)\n",
    "    else:\n",
    "        return fetch_batch_size_random(X_test,  Y_test,  batch_size)\n",
    "\n",
    "def generate_data_keras_batch(isTrain, batch_size):\n",
    "    global Y_train\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global Y_test\n",
    "    \n",
    "    if len(Y_test) == 0:\n",
    "        X_train, Y_train, X_test, Y_test = prepare_data(INPUT_SEQ_LENGTH, OUTPUT_SEQ_LENGTH, sliding_window=True, step_size=5)\n",
    "\n",
    "    if isTrain:\n",
    "        return fetch_batch_size_random_keras(X_train, Y_train, batch_size)\n",
    "    else:\n",
    "        return fetch_batch_size_random_keras(X_test, Y_test, batch_size)\n",
    "\n",
    "def generate_data_keras(input_seq_length, output_seq_length, step_size=5):\n",
    "    INPUT_SEQ_LENGTH = input_seq_length\n",
    "    OUTPUT_SEQ_LENGTH = output_seq_length\n",
    "    X_train, Y_train, X_test, Y_test = prepare_data(input_seq_length, output_seq_length, sliding_window=True, step_size=step_size)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded USDT_BTC\n",
      "MIN: Thu Feb 19 17:00:00 2015\n",
      "MAX: Thu Jul 12 10:00:00 2018\n",
      "Successfully downloaded USDT_LTC\n",
      "MIN: Sat Mar  7 21:00:00 2015\n",
      "MAX: Thu Jul 12 10:00:00 2018\n",
      "Successfully downloaded USDT_ETH\n",
      "MIN: Sat Aug  8 06:00:00 2015\n",
      "MAX: Thu Jul 12 10:00:00 2018\n",
      "Successfully downloaded USDT_XRP\n",
      "MIN: Fri Feb 20 21:00:00 2015\n",
      "MAX: Thu Jul 12 10:00:00 2018\n",
      "min date: Sat Aug  8 06:00:00 2015\n"
     ]
    }
   ],
   "source": [
    "download_data(period=14400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5471 entries, 0 to 5470\n",
      "Data columns (total 51 columns):\n",
      "btc_close                                          5471 non-null float64\n",
      "btc_high                                           5471 non-null float64\n",
      "btc_low                                            5471 non-null float64\n",
      "btc_open                                           5471 non-null float64\n",
      "btc_quoteVolume                                    5471 non-null float64\n",
      "btc_volume                                         5471 non-null float64\n",
      "btc_weightedAverage                                5471 non-null float64\n",
      "eth_close                                          5471 non-null float64\n",
      "eth_high                                           5471 non-null float64\n",
      "eth_low                                            5471 non-null float64\n",
      "eth_open                                           5471 non-null float64\n",
      "eth_quoteVolume                                    5471 non-null float64\n",
      "eth_volume                                         5471 non-null float64\n",
      "eth_weightedAverage                                5471 non-null float64\n",
      "ltc_close                                          5471 non-null float64\n",
      "ltc_high                                           5471 non-null float64\n",
      "ltc_low                                            5471 non-null float64\n",
      "ltc_open                                           5471 non-null float64\n",
      "ltc_quoteVolume                                    5471 non-null float64\n",
      "ltc_volume                                         5471 non-null float64\n",
      "ltc_weightedAverage                                5471 non-null float64\n",
      "xrp_close                                          5471 non-null float64\n",
      "xrp_high                                           5471 non-null float64\n",
      "xrp_low                                            5471 non-null float64\n",
      "xrp_open                                           5471 non-null float64\n",
      "xrp_quoteVolume                                    5471 non-null float64\n",
      "xrp_volume                                         5471 non-null float64\n",
      "xrp_weightedAverage                                5471 non-null float64\n",
      "date                                               5471 non-null int64\n",
      "vader_sent                                         5471 non-null float64\n",
      "favorites                                          5471 non-null float64\n",
      "lex_sent                                           5471 non-null float64\n",
      "cnn_sent                                           5471 non-null float64\n",
      "cnn_pos_neg                                        5471 non-null float64\n",
      "avg-block-size                                     5471 non-null float64\n",
      "cost-per-transaction                               5471 non-null float64\n",
      "difficulty                                         5471 non-null float64\n",
      "hash-rate                                          5471 non-null float64\n",
      "market-cap                                         5471 non-null float64\n",
      "median-confirmation-time                           5471 non-null float64\n",
      "transaction-fees                                   5471 non-null float64\n",
      "transaction-fees-usd                               5471 non-null float64\n",
      "n-transactions-per-block                           5471 non-null float64\n",
      "miners-revenue                                     5471 non-null float64\n",
      "n-unique-addresses                                 5471 non-null float64\n",
      "n-transactions                                     5471 non-null float64\n",
      "n-transactions-excluding-popular                   5471 non-null float64\n",
      "n-transactions-excluding-chains-longer-than-100    5471 non-null float64\n",
      "output-volume                                      5471 non-null float64\n",
      "estimated-transaction-volume                       5471 non-null float64\n",
      "estimated-transaction-volume-usd                   5471 non-null float64\n",
      "dtypes: float64(50), int64(1)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "load_data().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
