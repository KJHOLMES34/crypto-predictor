{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "DATA_MARKET = 'data/poloniex/'\n",
    "DATA_TWITTER = 'data/twitter/sentiment/'\n",
    "\n",
    "class PastSampler:\n",
    "\n",
    "    def __init__(self, N, K, sliding_window = True):\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.sliding_window = sliding_window\n",
    " \n",
    "    def transform(self, A):\n",
    "        M = self.N + self.K     #Number of samples per row (sample + target)\n",
    "        #indexes\n",
    "        if self.sliding_window:\n",
    "            I = np.arange(M) + np.arange(A.shape[0] - M + 1).reshape(-1, 1)\n",
    "        else:\n",
    "            if A.shape[0]%M == 0:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0],M).reshape(-1,1)\n",
    "                \n",
    "            else:\n",
    "                I = np.arange(M)+np.arange(0,A.shape[0] -M,M).reshape(-1,1)    \n",
    "        #print(I)\n",
    "        print(I.shape)\n",
    "        \n",
    "        B = A[I].reshape(-1, M * A.shape[1], A.shape[2])\n",
    "        ci = self.N * A.shape[1]    #Number of features per sample\n",
    "        print('ci', ci)\n",
    "        print('B shape', B.shape)\n",
    "        return B[:, :ci], B[:, ci:] #Sample matrix, Target matrix\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"    \n",
    "    price_data = pd.read_pickle(DATA_MARKET + 'combined.pkl')\n",
    "    sentiment_data = pd.read_pickle(DATA_TWITTER + 'btc_expanded.pkl')\n",
    "    \n",
    "    min_date = min(sentiment_data['date'])\n",
    "    max_date = max(sentiment_data['date'])\n",
    "    \n",
    "    price_data = price_data.query('@min_date <= date <= @max_date')\n",
    "    \n",
    "    return pd.merge(price_data, sentiment_data, how='inner', left_on='date', right_on='date')\n",
    "\n",
    "def normalize(X, Y=None):\n",
    "    \"\"\"\n",
    "    Normalise X and Y according to the mean and standard deviation of the X values only.\n",
    "    \"\"\"\n",
    "    # # It would be possible to normalize with last rather than mean, such as:\n",
    "    # lasts = np.expand_dims(X[:, -1, :], axis=1)\n",
    "    # assert (lasts[:, :] == X[:, -1, :]).all(), \"{}, {}, {}. {}\".format(lasts[:, :].shape, X[:, -1, :].shape, lasts[:, :], X[:, -1, :])\n",
    "    mean = np.expand_dims(np.average(X, axis=1) + 0.00001, axis=1)\n",
    "    stddev = np.expand_dims(np.std(X, axis=1) + 0.00001, axis=1)\n",
    "    print (mean.shape, stddev.shape)\n",
    "    # print (X.shape, Y.shape)\n",
    "    X = X - mean\n",
    "    X = X / (2.5 * stddev)\n",
    "    if Y is not None:\n",
    "        assert Y.shape == X.shape, (Y.shape, X.shape)\n",
    "        Y = Y - mean\n",
    "        Y = Y / (2.5 * stddev)\n",
    "        return X, Y\n",
    "    return X\n",
    "\n",
    "def fetch_batch_size_random(X, Y, batch_size):\n",
    "    \"\"\"\n",
    "    Returns randomly an aligned batch_size of X and Y among all examples.\n",
    "    The external dimension of X and Y must be the batch size (eg: 1 column = 1 example).\n",
    "    X and Y can be N-dimensional.\n",
    "    \"\"\"\n",
    "    assert X.shape == Y.shape, (X.shape, Y.shape)\n",
    "    idxes = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_out = np.array(X[idxes]).transpose((1, 0, 2))\n",
    "    Y_out = np.array(Y[idxes]).transpose((1, 0, 2))\n",
    "    return X_out, Y_out\n",
    "\n",
    "def generate_data(isTrain, batch_size):\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    # 40 pas values for encoder, 40 after for decoder's predictions.\n",
    "    input_seq_length = 864 # 3 Days - 3*24*60/5\n",
    "    output_seq_length = 24 # 2 hours\n",
    "    split = 0.85\n",
    "\n",
    "    global Y_train\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global Y_test\n",
    "    # First load, with memoization:\n",
    "    if len(Y_test) == 0:\n",
    "        # Dejan\n",
    "        data = load_data()\n",
    "        ps = PastSampler(input_seq_length, output_seq_length, sliding_window=True)\n",
    "\n",
    "        # All data, aligned:\n",
    "        X, Y = ps.transform(data.as_matrix()[:,None,:])\n",
    "        #X, Y = normalize(X, Y)\n",
    "\n",
    "        # Split 85-15:\n",
    "        X_train = X[:int(len(X) * split)]\n",
    "        Y_train = Y[:int(len(Y) * split)]\n",
    "        X_test = X[int(len(X) * split):]\n",
    "        Y_test = Y[int(len(Y) * split):]\n",
    "\n",
    "    if isTrain:\n",
    "        return fetch_batch_size_random(X_train, Y_train, batch_size)\n",
    "    else:\n",
    "        return fetch_batch_size_random(X_test,  Y_test,  batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 888)\n",
      "ci 864\n",
      "B shape (255, 888, 20)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "ps = PastSampler(864, 24, sliding_window=False)\n",
    "X, Y = ps.transform(data.as_matrix()[:,None,:])\n",
    "print(X.dtype.names)\n",
    "#X, Y = normalize(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
